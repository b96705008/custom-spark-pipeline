{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tykuo_spark_model.imputer import TykuoImputer\n",
    "from tykuo_spark_model.onehot import StringDisassembler\n",
    "from tykuo_spark_model.vec_disamb import VectorDisassembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyspark.sql.session.SparkSession at 0x11184d090>,\n",
       " <pyspark.context.SparkContext at 0x11175be50>,\n",
       " <bound method SparkSession.sql of <pyspark.sql.session.SparkSession object at 0x11184d090>>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark, sc, sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sc.parallelize([\n",
    "        (1, 'a', 300, None), \n",
    "        (2, 'a', 400, 30.3), \n",
    "        (3, 'b', None, 27.8), \n",
    "        (4, 'c', 600, 31.2), \n",
    "        (5, None, 700, 32.5)]) \\\n",
    "    .toDF([\"id\", \"x1\", 'x2', 'x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+\n",
      "| id|  x1|  x2|  x3|\n",
      "+---+----+----+----+\n",
      "|  1|   a| 300|null|\n",
      "|  2|   a| 400|30.3|\n",
      "|  3|   b|null|27.8|\n",
      "|  4|   c| 600|31.2|\n",
      "|  5|null| 700|32.5|\n",
      "+---+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    TykuoImputer().setInputCol('x1').setOutputCol('x1_imp').setStrategy('mode'),\n",
    "    TykuoImputer().setInputCol('x2').setOutputCol('x2_imp').setStrategy('median'),\n",
    "    TykuoImputer().setInputCol('x3').setOutputCol('x3_imp').setStrategy('mean'),\n",
    "    StringDisassembler().setInputCol('x1_imp'),\n",
    "    VectorAssembler(inputCols=['x2_imp', 'x3_imp'], outputCol='features'),\n",
    "    StandardScaler(inputCol='features', outputCol='scaledFeatures'),\n",
    "    VectorDisassembler().setInputCol('scaledFeatures').setOutputCols(['x2_scaled', 'x3_scaled'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+------+------+------+-----------+-----------+-----------+-------------+--------------------+------------------+------------------+\n",
      "| id|  x1|  x2|  x3|x1_imp|x2_imp|x3_imp|is_x1_imp_a|is_x1_imp_c|is_x1_imp_b|     features|      scaledFeatures|         x2_scaled|         x3_scaled|\n",
      "+---+----+----+----+------+------+------+-----------+-----------+-----------+-------------+--------------------+------------------+------------------+\n",
      "|  1|   a| 300|null|     a| 300.0| 30.45|        1.0|        0.0|        0.0|[300.0,30.45]|[1.82574185835055...|1.8257418583505538|17.721168042270467|\n",
      "|  2|   a| 400|30.3|     a| 400.0|  30.3|        1.0|        0.0|        0.0| [400.0,30.3]|[2.43432247780073...|2.4343224778007384|17.633871647973567|\n",
      "|  3|   b|null|27.8|     b| 400.0|  27.8|        0.0|        0.0|        1.0| [400.0,27.8]|[2.43432247780073...|2.4343224778007384|16.178931743025252|\n",
      "|  4|   c| 600|31.2|     c| 600.0|  31.2|        0.0|        1.0|        0.0| [600.0,31.2]|[3.65148371670110...|3.6514837167011076| 18.15765001375496|\n",
      "|  5|null| 700|32.5|     a| 700.0|  32.5|        1.0|        0.0|        0.0| [700.0,32.5]|[4.26006433615129...| 4.260064336151292|18.914218764328083|\n",
      "+---+----+----+----+------+------+------+-----------+-----------+-----------+-------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
